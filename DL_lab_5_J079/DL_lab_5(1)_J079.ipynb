{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL lab 5(1) J079.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiBctXbMJ+jpjj0Pt8qSk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmayj23/DL/blob/main/DL_lab_5_J079/DL_lab_5(1)_J079.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XAjKiz6IWbg"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QiWX5A1I9yv"
      },
      "source": [
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
        "    as illustrated in Figure 1.\n",
        "    \n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    \n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = 0)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_pad"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "HfhzRF7ZJCuu",
        "outputId": "5ef1d0ad-2e32-4d87-ef05-579ec0f39761"
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(4, 3, 3, 2)\n",
        "x_pad = zero_pad(x, 2)\n",
        "print (\"x.shape =\", x.shape)\n",
        "print (\"x_pad.shape =\", x_pad.shape)\n",
        "print (\"x[1,1] =\", x[1,1])\n",
        "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2)\n",
        "axarr[0].set_title('x')\n",
        "axarr[0].imshow(x[0,:,:,0])\n",
        "axarr[1].set_title('x_pad')\n",
        "axarr[1].imshow(x_pad[0,:,:,0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.shape = (4, 3, 3, 2)\n",
            "x_pad.shape = (4, 7, 7, 2)\n",
            "x[1,1] = [[ 0.90085595 -0.68372786]\n",
            " [-0.12289023 -0.93576943]\n",
            " [-0.26788808  0.53035547]]\n",
            "x_pad[1,1] = [[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4ac8545d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHfLaiI-JFIJ"
      },
      "source": [
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
        "    of the previous layer.\n",
        "    \n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
        "    s = a_slice_prev *W\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = np.sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    Z = Z + float(b)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return Z"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfjrRwfeJLyq",
        "outputId": "471e7ba3-e1df-45d8-8e13-cc2bd4c2f446"
      },
      "source": [
        "np.random.seed(1)\n",
        "a_slice_prev = np.random.randn(4, 4, 3)\n",
        "W = np.random.randn(4, 4, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "\n",
        "Z = conv_single_step(a_slice_prev, W, b)\n",
        "print(\"Z =\", Z)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z = -6.999089450680221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYuxT4X0JQ0I"
      },
      "source": [
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape (≈1 line)\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
        "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
        "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
        "    \n",
        "    # Initialize the output volume Z with zeros. (≈1 line)\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    \n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i,:,:,:]                   # Select ith training example's padded activation\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "                    #print (a_slice_prev.shape)\n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[..., c], b[..., c])\n",
        "                                        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache# GRADED FUNCTION: conv_forward\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "        \n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape (≈1 line)\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    \n",
        "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
        "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
        "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
        "    \n",
        "    # Initialize the output volume Z with zeros. (≈1 line)\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    \n",
        "    for i in range(m):                               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i,:,:,:]                   # Select ith training example's padded activation\n",
        "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "                    #print (a_slice_prev.shape)\n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[..., c], b[..., c])\n",
        "                                        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fD932NYJRcg",
        "outputId": "ccb1acbf-7718-4341-e470-75e396d2c169"
      },
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10,4,4,3)\n",
        "W = np.random.randn(2,2,3,8)\n",
        "b = np.random.randn(1,1,1,8)\n",
        "hparameters = {\"pad\" : 2,\n",
        "               \"stride\": 2}\n",
        "\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "print(\"Z's mean =\", np.mean(Z))\n",
        "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
        "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z's mean = 0.048995203528855794\n",
            "Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
            "  5.18531798  8.75898442]\n",
            "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQJgdFFQJV-Y"
      },
      "source": [
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the forward pass of the pooling layer\n",
        "    \n",
        "    Arguments:\n",
        "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve dimensions from the input shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve hyperparameters from \"hparameters\"\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = hparameters[\"stride\"]\n",
        "    \n",
        "    # Define the dimensions of the output\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "    \n",
        "    # Initialize output matrix A\n",
        "    A = np.zeros((m, n_H, n_W, n_C))              \n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    for i in range(m):                            # loop over the training examples\n",
        "        for h in range(n_H):                      # loop on the vertical axis of the output volume\n",
        "            for w in range(n_W):                  # loop on the horizontal axis of the output volume\n",
        "                for c in range (n_C):             # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                    \n",
        "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                    \n",
        "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
        "    cache = (A_prev, hparameters)\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(A.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    return A, cache\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz0aY2zwJbq8",
        "outputId": "fea37fb3-6974-405a-ff2f-4c9549c89c12"
      },
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 4, 4, 3)\n",
        "hparameters = {\"stride\" : 2, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(\"mode = max\")\n",
        "print(\"A =\", A)\n",
        "print()\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print(\"A =\", A)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mode = max\n",
            "A = [[[[1.74481176 0.86540763 1.13376944]]]\n",
            "\n",
            "\n",
            " [[[1.13162939 1.51981682 2.18557541]]]]\n",
            "\n",
            "mode = average\n",
            "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
            "\n",
            "\n",
            " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mt5O4CAJhYL"
      },
      "source": [
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a convolution function\n",
        "    \n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          numpy array of shape (1, 1, 1, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    \n",
        "    # Retrieve dimensions from W's shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    \n",
        "    # Retrieve information from \"hparameters\"\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "    \n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    \n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "    db = np.zeros((1, 1, 1, n_C))\n",
        "\n",
        "\n",
        "   # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "    \n",
        "    for i in range(m):                          # loop over the training examples\n",
        "        \n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "        \n",
        "        for h in range(n_H):                    # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):                # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):            # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                    \n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                    \n",
        "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcqt7ZKFJjW5",
        "outputId": "a478fb3d-2eb9-4688-c711-65ed7601fda9"
      },
      "source": [
        "np.random.seed(1)\n",
        "dA, dW, db = conv_backward(Z, cache_conv)\n",
        "print(\"dA_mean =\", np.mean(dA))\n",
        "print(\"dW_mean =\", np.mean(dW))\n",
        "print(\"db_mean =\", np.mean(db))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dA_mean = 1.4524377775388075\n",
            "dW_mean = 1.7269914583139097\n",
            "db_mean = 7.839232564616838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnu3LOo9JmKE"
      },
      "source": [
        "def create_mask_from_window(x):\n",
        "    \"\"\"\n",
        "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- Array of shape (f, f)\n",
        "    \n",
        "    Returns:\n",
        "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈1 line)\n",
        "    mask = (x == np.max(x))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return mask"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MWsegz-Jmux",
        "outputId": "d2f9821c-599f-4141-8f27-0fcb3079d5e8"
      },
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(2,3)\n",
        "mask = create_mask_from_window(x)\n",
        "print('x = ', x)\n",
        "print(\"mask = \", mask)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
            " [-1.07296862  0.86540763 -2.3015387 ]]\n",
            "mask =  [[ True False False]\n",
            " [False False False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Hnn3JPJrk-"
      },
      "source": [
        "def distribute_value(dz, shape):\n",
        "    \"\"\"\n",
        "    Distributes the input value in the matrix of dimension shape\n",
        "    \n",
        "    Arguments:\n",
        "    dz -- input scalar\n",
        "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "    \n",
        "    Returns:\n",
        "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from shape (≈1 line)\n",
        "    (n_H, n_W) = shape\n",
        "    \n",
        "    # Compute the value to distribute on the matrix (≈1 line)\n",
        "    average = dz / (n_H * n_W)\n",
        "    \n",
        "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
        "    a = np.ones(shape) * average\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return a"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJho77NUJtDK",
        "outputId": "2962d046-8feb-46f3-fdb7-122a3f8d804d"
      },
      "source": [
        "\n",
        "a = distribute_value(2, (2,2))\n",
        "print('distributed value =', a)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distributed value = [[0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3cFCoJIJvI6"
      },
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the backward pass of the pooling layer\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
        "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Retrieve information from cache (≈1 line)\n",
        "    (A_prev, hparameters) = cache\n",
        "    \n",
        "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "    \n",
        "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "    \n",
        "    # Initialize dA_prev with zeros (≈1 line)\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    \n",
        "    for i in range(m):                          # loop over the training examples\n",
        "        \n",
        "        # select training example from A_prev (≈1 line)\n",
        "        a_prev = A_prev[i]\n",
        "        \n",
        "        for h in range(n_H):                    # loop on the vertical axis\n",
        "            for w in range(n_W):                # loop on the horizontal axis\n",
        "                for c in range(n_C):            # loop over the channels (depth)\n",
        "                     # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                    \n",
        "                    # Compute the backward propagation in both modes.\n",
        "                    if mode == \"max\":\n",
        "                        \n",
        "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                        # Create the mask from a_prev_slice (≈1 line)\n",
        "                        mask = create_mask_from_window(a_prev_slice)\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
        "                        \n",
        "                    elif mode == \"average\":\n",
        "                        \n",
        "                        # Get the value a from dA (≈1 line)\n",
        "                        da = dA[i, h, w, c]\n",
        "                        # Define the shape of the filter as fxf (≈1 line)\n",
        "                        shape = (f, f)\n",
        "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
        "                        \n",
        "    ### END CODE ###\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "    \n",
        "    return dA_prev\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "    \n",
        "    return dA_prev"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEEX6g7xJ0Dp",
        "outputId": "7db8c834-f17a-4b72-93ab-7b7569804326"
      },
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(5, 5, 3, 2)\n",
        "hparameters = {\"stride\" : 1, \"f\": 2}\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "dA = np.random.randn(5, 4, 2, 2)\n",
        "\n",
        "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
        "print()\n",
        "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev[1,1] = ', dA_prev[1,1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mode = max\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev[1,1] =  [[ 0.          0.        ]\n",
            " [ 5.05844394 -1.68282702]\n",
            " [ 0.          0.        ]]\n",
            "\n",
            "mode = average\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
            " [ 1.26461098 -0.25749373]\n",
            " [ 1.17975636 -0.53624893]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}